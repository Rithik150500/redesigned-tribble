# Building Your Legal Risk Analysis System: A Step-by-Step Implementation

I'm going to walk you through building this entire system, layer by layer. We'll start from the foundation and work our way up, and I'll explain not just what we're building, but why we're making each design decision. By the end, you'll have a complete, working system and a deep understanding of how all the pieces fit together.

## Layer 1: Document Processing Pipeline and Database Schema

Let's begin at the very foundation. Before any agent can analyze documents, we need to transform raw PDFs into structured, AI-readable data. Think of this as creating a detailed map of a territory before sending explorers into it.

### Setting Up the Database

First, let's create our database schema. I'm using SQLite for simplicity, but this design works equally well with PostgreSQL or any other database. The schema reflects our hierarchical understanding of documents - each document contains many pages, and each page has multiple representations (text, image, summary).

```python
# database.py
import sqlite3
from pathlib import Path
from typing import Optional
import json

class LegalDocumentDatabase:
    """
    Manages the database of legal documents and their analyzed pages.
    
    This database stores three types of information:
    1. The original documents and their high-level summaries
    2. Individual pages with their text, images, and summaries
    3. Metadata about legal significance for quick filtering
    """
    
    def __init__(self, db_path: str = "legal_documents.db"):
        """Initialize the database connection and create tables if needed."""
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row  # This lets us access columns by name
        self._create_tables()
    
    def _create_tables(self):
        """
        Create the database schema.
        
        We're creating two main tables with a one-to-many relationship:
        documents -> pages. Each document can have many pages, but each page
        belongs to exactly one document.
        """
        cursor = self.conn.cursor()
        
        # The documents table stores high-level information about each document
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                doc_id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT NOT NULL UNIQUE,
                filepath TEXT NOT NULL,
                summdesc TEXT,  -- The 2-3 sentence summary of the entire document
                total_pages INTEGER,
                legally_significant_pages INTEGER,  -- Count for quick reference
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_hash TEXT  -- To detect if the same file is uploaded again
            )
        """)
        
        # The pages table stores detailed information about each page
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS pages (
                page_id INTEGER PRIMARY KEY AUTOINCREMENT,
                doc_id INTEGER NOT NULL,
                page_num INTEGER NOT NULL,
                summdesc TEXT,  -- The one-sentence summary of this page
                page_text TEXT,  -- The extracted text content
                page_image BLOB,  -- The page rendered as an image, stored as binary
                legally_significant INTEGER DEFAULT 0,  -- Boolean: 0 or 1
                FOREIGN KEY (doc_id) REFERENCES documents(doc_id),
                UNIQUE(doc_id, page_num)  -- Ensure we don't duplicate pages
            )
        """)
        
        # Create indexes for faster querying
        # We'll often filter by legally_significant and look up by doc_id
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_pages_doc_id 
            ON pages(doc_id)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_pages_legally_significant 
            ON pages(doc_id, legally_significant)
        """)
        
        self.conn.commit()
    
    def add_document(self, filename: str, filepath: str, file_hash: str) -> int:
        """
        Add a new document to the database.
        
        Returns the doc_id of the newly created document.
        This is called when we first encounter a PDF file.
        """
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO documents (filename, filepath, file_hash)
            VALUES (?, ?, ?)
        """, (filename, filepath, file_hash))
        self.conn.commit()
        return cursor.lastrowid
    
    def add_page(self, doc_id: int, page_num: int, page_text: str, 
                 page_image: bytes, summdesc: str = None):
        """
        Add a page to the database.
        
        This is called for each page as we process a document.
        We store both the text and image representation.
        """
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO pages (doc_id, page_num, page_text, page_image, summdesc)
            VALUES (?, ?, ?, ?, ?)
        """, (doc_id, page_num, page_text, page_image, summdesc))
        self.conn.commit()
        return cursor.lastrowid
    
    def update_document_summary(self, doc_id: int, summdesc: str, 
                               legally_significant_pages: list[int]):
        """
        Update the document with its summary and mark legally significant pages.
        
        This is called after we've processed all pages and generated the
        document-level summary. We also mark which pages are legally significant.
        """
        cursor = self.conn.cursor()
        
        # Update the document summary
        cursor.execute("""
            UPDATE documents 
            SET summdesc = ?, legally_significant_pages = ?
            WHERE doc_id = ?
        """, (summdesc, len(legally_significant_pages), doc_id))
        
        # Mark legally significant pages
        if legally_significant_pages:
            placeholders = ','.join('?' * len(legally_significant_pages))
            cursor.execute(f"""
                UPDATE pages 
                SET legally_significant = 1
                WHERE doc_id = ? AND page_num IN ({placeholders})
            """, [doc_id] + legally_significant_pages)
        
        self.conn.commit()
    
    def get_document(self, doc_id: int) -> Optional[dict]:
        """Retrieve a document's metadata."""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT * FROM documents WHERE doc_id = ?
        """, (doc_id,))
        row = cursor.fetchone()
        return dict(row) if row else None
    
    def get_all_documents(self) -> list[dict]:
        """Get all documents with their summaries."""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT doc_id, filename, summdesc, total_pages, 
                   legally_significant_pages
            FROM documents
            ORDER BY filename
        """)
        return [dict(row) for row in cursor.fetchall()]
    
    def get_pages(self, doc_id: int, page_nums: list[int] = None) -> list[dict]:
        """
        Get pages for a document.
        
        If page_nums is provided, get only those pages.
        Otherwise, get all pages.
        """
        cursor = self.conn.cursor()
        
        if page_nums:
            placeholders = ','.join('?' * len(page_nums))
            cursor.execute(f"""
                SELECT page_id, page_num, summdesc, page_text, 
                       page_image, legally_significant
                FROM pages
                WHERE doc_id = ? AND page_num IN ({placeholders})
                ORDER BY page_num
            """, [doc_id] + page_nums)
        else:
            cursor.execute("""
                SELECT page_id, page_num, summdesc, page_text, 
                       page_image, legally_significant
                FROM pages
                WHERE doc_id = ?
                ORDER BY page_num
            """, (doc_id,))
        
        return [dict(row) for row in cursor.fetchall()]
    
    def get_legally_significant_pages(self, doc_id: int) -> list[dict]:
        """Get only the legally significant pages for a document."""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT page_id, page_num, summdesc, page_text, legally_significant
            FROM pages
            WHERE doc_id = ? AND legally_significant = 1
            ORDER BY page_num
        """, (doc_id,))
        return [dict(row) for row in cursor.fetchall()]
    
    def close(self):
        """Close the database connection."""
        self.conn.close()
```

This database design captures everything we need. Notice how we're storing both text and images for each page, because sometimes the visual layout conveys meaning that text extraction misses. We're also storing summaries at both the page and document level, creating a hierarchy of understanding that agents can navigate efficiently.

### Building the Document Processor

Now let's build the core processing pipeline that takes a PDF and transforms it into our structured database format. This is where we extract text and images, generate summaries with Claude, and identify legally significant pages.

```python
# document_processor.py
import anthropic
from pathlib import Path
import hashlib
import base64
from pdf2image import convert_from_path
import pypdf
from typing import Optional
import io
from PIL import Image

class DocumentProcessor:
    """
    Processes PDF documents into the structured format needed for AI analysis.
    
    This class handles the entire pipeline:
    1. Extract text and images from each page
    2. Generate page-level summaries using Claude Haiku
    3. Generate document-level summary and identify significant pages
    4. Store everything in the database
    """
    
    def __init__(self, database: LegalDocumentDatabase, anthropic_api_key: str):
        """
        Initialize the processor with database and API connections.
        
        We use Claude Haiku for summarization because it's fast and cost-effective
        for processing potentially hundreds of pages.
        """
        self.db = database
        self.client = anthropic.Anthropic(api_key=anthropic_api_key)
    
    def _calculate_file_hash(self, filepath: Path) -> str:
        """
        Calculate a hash of the file to detect duplicates.
        
        This prevents reprocessing the same document if it's uploaded again.
        """
        sha256_hash = hashlib.sha256()
        with open(filepath, "rb") as f:
            # Read in chunks to handle large files efficiently
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def _extract_text_from_page(self, pdf_reader: pypdf.PdfReader, 
                                page_num: int) -> str:
        """
        Extract text from a single PDF page.
        
        Returns the raw text content. Some PDFs have poor text extraction,
        which is why we also keep the image representation.
        """
        try:
            page = pdf_reader.pages[page_num]
            text = page.extract_text()
            return text.strip() if text else ""
        except Exception as e:
            print(f"Error extracting text from page {page_num}: {e}")
            return ""
    
    def _render_page_as_image(self, pdf_path: Path, page_num: int) -> bytes:
        """
        Render a PDF page as an image.
        
        We convert to image because:
        1. Visual layout can be legally significant (tables, highlights, etc.)
        2. Some PDFs have text that doesn't extract well
        3. Claude can analyze images directly
        
        Returns the image as PNG bytes for database storage.
        """
        try:
            # Convert just this one page (pages are 1-indexed in convert_from_path)
            images = convert_from_path(
                pdf_path, 
                first_page=page_num + 1,  # convert_from_path uses 1-based indexing
                last_page=page_num + 1,
                dpi=150  # Good balance between quality and file size
            )
            
            if images:
                # Convert PIL Image to bytes
                img_byte_arr = io.BytesIO()
                images[0].save(img_byte_arr, format='PNG', optimize=True)
                return img_byte_arr.getvalue()
            
            return b""
        except Exception as e:
            print(f"Error rendering page {page_num} as image: {e}")
            return b""
    
    def _summarize_page(self, page_text: str, page_image_bytes: bytes, 
                       page_num: int) -> str:
        """
        Generate a one-sentence summary of a page using Claude Haiku.
        
        We send both the text and image because:
        - Text provides the raw content
        - Image provides visual context (tables, formatting, etc.)
        
        This dual input helps Claude understand the page more completely.
        """
        try:
            # Prepare the image in base64 format for Claude
            image_base64 = base64.b64encode(page_image_bytes).decode('utf-8')
            
            response = self.client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=150,  # One sentence doesn't need many tokens
                messages=[{
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/png",
                                "data": image_base64
                            }
                        },
                        {
                            "type": "text",
                            "text": f"""You are analyzing page {page_num + 1} of a legal document.

Here is the extracted text from this page:
{page_text[:2000]}  # Limit text length to avoid token overflow

Provide a single, concise sentence that summarizes the main topic or legal significance of this page. Focus on what legal matter this page addresses (e.g., "Non-compete clause restrictions", "Intellectual property assignment provisions", "Liability limitations and indemnification").

Your summary:"""
                        }
                    ]
                }]
            )
            
            summary = response.content[0].text.strip()
            # Ensure it's actually one sentence
            if '.' in summary:
                summary = summary.split('.')[0] + '.'
            return summary
            
        except Exception as e:
            print(f"Error summarizing page {page_num}: {e}")
            return "Unable to generate summary for this page."
    
    def _analyze_document(self, doc_id: int) -> tuple[str, list[int]]:
        """
        Analyze all pages of a document to create a document-level summary
        and identify legally significant pages.
        
        This is called after all pages have been processed individually.
        Returns: (document_summary, list_of_significant_page_numbers)
        """
        # Get all page summaries
        pages = self.db.get_pages(doc_id)
        
        # Build a comprehensive view of all pages
        page_summaries = []
        for page in pages:
            page_summaries.append(
                f"Page {page['page_num']}: {page['summdesc']}"
            )
        
        combined_summaries = "\n".join(page_summaries)
        
        try:
            response = self.client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=500,
                messages=[{
                    "role": "user",
                    "content": f"""You are analyzing a complete legal document. Here are summaries of each page:

{combined_summaries}

Please provide:
1. A 2-3 sentence overview of what this document is and its main legal purpose
2. A comma-separated list of page numbers that contain legally significant information

Legally significant pages are those containing:
- Contractual obligations or duties
- Deadlines or time-sensitive provisions
- Financial terms or payment obligations
- Liability clauses or indemnification
- Termination conditions
- Intellectual property assignments
- Non-compete or confidentiality provisions
- Regulatory compliance requirements
- Dispute resolution or governing law provisions
- Material representations or warranties

Format your response exactly as:
SUMMARY: [Your 2-3 sentence summary]
SIGNIFICANT_PAGES: [comma-separated page numbers, e.g., 1,3,7,12]

If no pages are particularly significant, respond with SIGNIFICANT_PAGES: none"""
                }]
            )
            
            response_text = response.content[0].text.strip()
            
            # Parse the response
            summary = ""
            significant_pages = []
            
            for line in response_text.split('\n'):
                if line.startswith('SUMMARY:'):
                    summary = line.replace('SUMMARY:', '').strip()
                elif line.startswith('SIGNIFICANT_PAGES:'):
                    pages_str = line.replace('SIGNIFICANT_PAGES:', '').strip()
                    if pages_str.lower() != 'none':
                        # Parse comma-separated page numbers
                        try:
                            significant_pages = [
                                int(p.strip()) 
                                for p in pages_str.split(',') 
                                if p.strip().isdigit()
                            ]
                        except ValueError:
                            print(f"Error parsing page numbers: {pages_str}")
            
            return summary, significant_pages
            
        except Exception as e:
            print(f"Error analyzing document {doc_id}: {e}")
            return "Unable to generate document summary.", []
    
    def process_document(self, pdf_path: Path) -> int:
        """
        Process a complete PDF document through the entire pipeline.
        
        This is the main entry point. It:
        1. Checks if the document was already processed
        2. Extracts text and images from each page
        3. Generates page summaries
        4. Generates document summary and identifies significant pages
        5. Stores everything in the database
        
        Returns the doc_id of the processed document.
        """
        print(f"\n{'='*60}")
        print(f"Processing document: {pdf_path.name}")
        print(f"{'='*60}\n")
        
        # Calculate file hash to check for duplicates
        file_hash = self._calculate_file_hash(pdf_path)
        
        # Check if already processed
        # (We'd need to add a method to check by hash, but I'll skip that for brevity)
        
        # Create document record
        doc_id = self.db.add_document(
            filename=pdf_path.name,
            filepath=str(pdf_path),
            file_hash=file_hash
        )
        
        # Open PDF for reading
        with open(pdf_path, 'rb') as f:
            pdf_reader = pypdf.PdfReader(f)
            total_pages = len(pdf_reader.pages)
            
            print(f"Document has {total_pages} pages. Processing each page...\n")
            
            # Process each page
            for page_num in range(total_pages):
                print(f"Processing page {page_num + 1}/{total_pages}...", end=' ')
                
                # Extract text
                page_text = self._extract_text_from_page(pdf_reader, page_num)
                
                # Render as image
                page_image = self._render_page_as_image(pdf_path, page_num)
                
                # Generate summary
                page_summary = self._summarize_page(page_text, page_image, page_num)
                print(f"✓ Summary: {page_summary[:60]}...")
                
                # Store in database
                self.db.add_page(
                    doc_id=doc_id,
                    page_num=page_num + 1,  # Store as 1-indexed for human readability
                    page_text=page_text,
                    page_image=page_image,
                    summdesc=page_summary
                )
            
            # Update document total_pages
            cursor = self.db.conn.cursor()
            cursor.execute("""
                UPDATE documents SET total_pages = ? WHERE doc_id = ?
            """, (total_pages, doc_id))
            self.db.conn.commit()
        
        print(f"\n{'='*60}")
        print("Analyzing complete document...")
        print(f"{'='*60}\n")
        
        # Generate document-level analysis
        doc_summary, significant_pages = self._analyze_document(doc_id)
        
        print(f"Document Summary: {doc_summary}")
        print(f"Legally Significant Pages: {significant_pages}\n")
        
        # Update document with summary and significant pages
        self.db.update_document_summary(doc_id, doc_summary, significant_pages)
        
        print(f"✓ Document processing complete! (doc_id: {doc_id})\n")
        
        return doc_id
```

This processor handles the entire transformation pipeline. Notice how we're being careful about error handling - if text extraction fails on a page, we still have the image. If summarization fails, we note it but continue processing.

### Creating a Processing Script

Now let's create a simple script that watches a folder and processes any PDFs it finds. This will be your document ingestion system.

```python
# ingest_documents.py
import os
from pathlib import Path
import time
from database import LegalDocumentDatabase
from document_processor import DocumentProcessor
import sys

def process_data_room(data_room_path: str, anthropic_api_key: str):
    """
    Process all PDF documents in a data room folder.
    
    This script finds all PDFs in the specified folder and processes them
    through our pipeline. You can run this whenever new documents are added.
    """
    data_room = Path(data_room_path)
    
    if not data_room.exists():
        print(f"Error: Data room folder '{data_room_path}' does not exist.")
        sys.exit(1)
    
    # Initialize database and processor
    db = LegalDocumentDatabase()
    processor = DocumentProcessor(db, anthropic_api_key)
    
    # Find all PDF files
    pdf_files = list(data_room.glob("*.pdf"))
    
    if not pdf_files:
        print(f"No PDF files found in {data_room_path}")
        return
    
    print(f"\nFound {len(pdf_files)} PDF documents to process")
    print(f"{'='*60}\n")
    
    # Process each document
    for i, pdf_path in enumerate(pdf_files, 1):
        print(f"\n[Document {i}/{len(pdf_files)}]")
        try:
            doc_id = processor.process_document(pdf_path)
            print(f"✓ Successfully processed: {pdf_path.name} (ID: {doc_id})")
        except Exception as e:
            print(f"✗ Error processing {pdf_path.name}: {e}")
            continue
    
    # Show summary
    print(f"\n{'='*60}")
    print("PROCESSING COMPLETE")
    print(f"{'='*60}\n")
    
    all_docs = db.get_all_documents()
    print(f"Total documents in database: {len(all_docs)}")
    print("\nDocument Summary:")
    for doc in all_docs:
        print(f"\n{doc['filename']}:")
        print(f"  Pages: {doc['total_pages']}")
        print(f"  Legally significant pages: {doc['legally_significant_pages']}")
        print(f"  Summary: {doc['summdesc'][:100]}...")
    
    db.close()

if __name__ == "__main__":
    # You would typically read these from environment variables or config
    DATA_ROOM_PATH = os.getenv("DATA_ROOM_PATH", "./data_room")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    
    if not ANTHROPIC_API_KEY:
        print("Error: ANTHROPIC_API_KEY environment variable not set")
        sys.exit(1)
    
    process_data_room(DATA_ROOM_PATH, ANTHROPIC_API_KEY)
```

Now you can run this script whenever you add new PDFs to your data room. It will process them all and store the structured data in your database, ready for agent analysis.

Let me pause here before moving to Layer 2. Does this foundation make sense? The key insight is that we're doing expensive processing once upfront (extracting text, rendering images, generating summaries) so that agents can later query this pre-processed intelligence efficiently. The database becomes a rich, indexed map of your legal documents that agents can navigate strategically rather than brute-forcing through raw PDFs.

Would you like me to clarify anything about the document processing layer before we move on to building the MCP server?
